{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/venkatanadikatla/pytorch/blob/main/RNN_with_SST_Better_Accuracy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 794
        },
        "id": "clpLVXLlIeYz",
        "outputId": "050ab0bb-61ee-4c06-f078-36dfcf49a38f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: torchtext 0.6.0\n",
            "Uninstalling torchtext-0.6.0:\n",
            "  Successfully uninstalled torchtext-0.6.0\n",
            "Collecting torchtext==0.6.0\n",
            "  Using cached torchtext-0.6.0-py3-none-any.whl (64 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torchtext==0.6.0) (4.66.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchtext==0.6.0) (2.31.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from torchtext==0.6.0) (2.3.1+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchtext==0.6.0) (1.25.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from torchtext==0.6.0) (1.16.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from torchtext==0.6.0) (0.1.99)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.6.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.6.0) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.6.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.6.0) (2024.7.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6.0) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6.0) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6.0) (1.13.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6.0) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6.0) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6.0) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6.0) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6.0) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6.0) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6.0) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6.0) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6.0) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6.0) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6.0) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6.0) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6.0) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6.0) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6.0) (2.3.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->torchtext==0.6.0) (12.5.82)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->torchtext==0.6.0) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->torchtext==0.6.0) (1.3.0)\n",
            "Installing collected packages: torchtext\n",
            "Successfully installed torchtext-0.6.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "torchtext"
                ]
              },
              "id": "a67b144aaeb04afc982639ebdf0fad4a"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip uninstall -y torchtext\n",
        "!pip install torchtext==0.6.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "cibFuZirIoEF"
      },
      "outputs": [],
      "source": [
        "import copy # this module provides functions to duplicate objects. It seems to be imported but not yet used in the code\n",
        "import torch # The MAIN PyTorch package\n",
        "from torch import nn # contains the essential modules for building NN in pytorch.\n",
        "from torch import optim # Provides optimization algorithms, such as SGD, Adam, etc\n",
        "import torchtext # A library for text processing that works well with pytorch (Currently a version 0.6.0 is being used in this code)\n",
        "from torchtext import data # A module in torchtext used for data handling\n",
        "from torchtext import datasets # provides datasets, including various NLP datasets.\n",
        "\n",
        "TEXT = data.Field(sequential=True, batch_first=True, lower=True) # Sequential = True indicates that the data consists of sequences.\n",
        "#Batch_first=True Ensure that batch dimenstion is the first dimension in the tensor. # lower=True Converts all the text to lowercase\n",
        "\n",
        "LABEL = data.LabelField() # A subclass of Field specifically for handling labels in a classification task.\n",
        "\n",
        "# load data splits\n",
        "train_data, val_data, test_data = datasets.SST.splits(TEXT, LABEL) #datasets.SST.splits - Loads the Standford Sentiment Treebank(SST) dataset and splits the dataset\n",
        "\n",
        "# build dictionary\n",
        "# build_vocab: Creates a mapping from tokens(words) to indices. This is essential for converting text data into numerical form that can be used by NN.\n",
        "TEXT.build_vocab(train_data) # Builds the vocabulary for the text field using the training data.\n",
        "LABEL.build_vocab(train_data)# Builds the vocabulary for the label field using the training data.\n",
        "\n",
        "# hyperparameters\n",
        "vocab_size = len(TEXT.vocab) # the size of the vocabulary (number of unique tokens in the training data)\n",
        "label_size = len(LABEL.vocab) # the number of unique labels (classes) in the traning data\n",
        "padding_idx = TEXT.vocab.stoi['<pad>'] # The index used for padding sequences to the same length\n",
        "embedding_dim = 300  # The size of the word embeddings (dense vector representation of words)\n",
        "hidden_dim = 512 # Size of the hidden layers in the model\n",
        "\n",
        "# build iterators\n",
        "train_iter, val_iter, test_iter = data.BucketIterator.splits(\n",
        "    (train_data, val_data, test_data),\n",
        "    batch_size=64)\n",
        "\n",
        "# Data.bucketiterator.splits - Creates iterators for the training, validation and test sets.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "DCtbhUqnIoGT"
      },
      "outputs": [],
      "source": [
        "class RNN (torch.nn.Module):\n",
        "  def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, padding_idx, dropout, num_layers=3):\n",
        "    super().__init__()\n",
        "    self.vocab_size = vocab_size\n",
        "    self.embedding_dim = embedding_dim\n",
        "    self.hidden_dim = hidden_dim\n",
        "    self.label_size = label_size\n",
        "    self.num_layers = num_layers\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    self.padding_idx = padding_idx\n",
        "    self.embedding = torch.nn.Embedding(vocab_size, embedding_dim,padding_idx = padding_idx)\n",
        "    # self.rnn = torch.nn.RNN(embedding_dim,hidden_dim, nonlinearity='relu',batch_first=True)\n",
        "    self.lstm = torch.nn.LSTM(embedding_dim,hidden_dim, dropout=dropout, num_layers=num_layers, batch_first=True, bidrectional=True)\n",
        "    self.fc = torch.nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "  def zero_state(self, batch_size):\n",
        "    # Implement the function, which returns an initial hidden state.\n",
        "    return (torch.zeros(self.num_layers, batch_size, self.hidden_dim),\n",
        "            torch.zeros(self.num_layers, batch_size, self.hidden_dim))\n",
        "\n",
        "\n",
        "  # def forward(self, text):\n",
        "  #   embedded = self.embedding(text)\n",
        "  #   batch_size = text.size(0)\n",
        "  #   h_0 = self.zero_state(batch_size).to(text.device)  # Ensure the hidden state is on the same device as the input\n",
        "  #   output, (hidden,cell) = self.lstm(embedded)\n",
        "  #   hidden = hidden[-1]\n",
        "  #   return self.fc(hidden)\n",
        "\n",
        "  def forward(self, text):\n",
        "    embedded = self.embedding(text)\n",
        "    batch_size = text.size(0)\n",
        "    h_0, c_0 = self.zero_state(batch_size)\n",
        "    h_0, c_0 = h_0.to(text.device), c_0.to(text.device)  # Ensure the hidden state is on the same device as the input\n",
        "    output, (hidden,cell) = self.lstm(embedded, (h_0, c_0))\n",
        "    hidden = hidden[-1]\n",
        "    return self.fc(hidden)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "d6ce0_3BIoI1"
      },
      "outputs": [],
      "source": [
        "def train_model(model,train_iter, optimizer, criterion, num_epochs =10):\n",
        "  model.train()\n",
        "  for epoch in range(num_epochs):\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for batch in train_iter:\n",
        "      optimizer.zero_grad()\n",
        "      text, labels = batch.text, batch.label\n",
        "      output = model(text)\n",
        "      loss = criterion(output, labels)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      epoch_loss += loss.item()\n",
        "      epoch_acc += (output.argmax(1) ==labels).sum().item()\n",
        "\n",
        "      _, predicted = torch.max(output.data,1)\n",
        "      total += labels.size(0)\n",
        "      correct += (predicted == labels).sum().item()\n",
        "\n",
        "    avg_loss = epoch_loss/len(train_iter)\n",
        "    avg_acc = epoch_acc/len(train_iter.dataset)\n",
        "\n",
        "    epoch_accuracy = 100*correct/total\n",
        "\n",
        "\n",
        "    print(f' Epoch {epoch+1}, Train Loss: {avg_loss}, Train Accuracy: {epoch_accuracy}')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "G-1jLhCCIoK3"
      },
      "outputs": [],
      "source": [
        "def eval_model(model, val_iter, criterion):\n",
        "  model.eval()\n",
        "  epoch_loss = 0\n",
        "  epoch_acc = 0\n",
        "  correct = 0\n",
        "  total = 0\n",
        "  for batch in val_iter:\n",
        "    text, labels = batch.text, batch.label\n",
        "    output = model(text)\n",
        "    loss = criterion(output, labels)\n",
        "    epoch_loss +=loss.item()\n",
        "    epoch_acc += (output.argmax(1)==labels).sum().item()\n",
        "    _, predicted = torch.max(output.data,1)\n",
        "    total += labels.size(0)\n",
        "    correct += (predicted == labels).sum().item()\n",
        "\n",
        "  avg_loss = epoch_loss/len(val_iter)\n",
        "  avg_acc = epoch_acc/len(val_iter.dataset)\n",
        "\n",
        "  epoch_accuracy = 100*avg_acc\n",
        "\n",
        "  print(f'Validation Loss: {avg_loss},  Validation Accuracy: {epoch_accuracy}')\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "lJlXVgETIoPW"
      },
      "outputs": [],
      "source": [
        "def test_model(model, test_iter, criterion):\n",
        "  model.eval()\n",
        "  epoch_loss = 0\n",
        "  epoch_acc = 0\n",
        "  correct = 0\n",
        "  total = 0\n",
        "  for batch in test_iter:\n",
        "    text, labels = batch.text, batch.label\n",
        "    output = model(text)\n",
        "    loss = criterion(output, labels)\n",
        "    epoch_loss +=loss.item()\n",
        "    epoch_acc += (output.argmax(1)==labels).sum().item()\n",
        "    _, predicted = torch.max(output.data,1)\n",
        "    total += labels.size(0)\n",
        "    correct += (predicted == labels).sum().item()\n",
        "\n",
        "  avg_loss = epoch_loss/len(test_iter)\n",
        "  avg_acc = epoch_acc/len(test_iter.dataset)\n",
        "\n",
        "  epoch_accuracy = 100*correct/total\n",
        "\n",
        "  print(f'Test Loss: {avg_loss},  Test Accuracy: {epoch_accuracy}')\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "2udT32qiIoR1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "a14048fa-fd70-4dad-abdd-08a7114c0f90"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'dropout' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-43-8c322769267e>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmanual_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# model = RNN(vocab_size,embedding_dim,hidden_dim,label_size, padding_idx, )\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m# optimizer = optim.Adam(model.parameters(), lr=0.0005)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'dropout' is not defined"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(42)\n",
        "# model = RNN(vocab_size,embedding_dim,hidden_dim,label_size, padding_idx, )\n",
        "model = RNN(vocab_size, embedding_dim, hidden_dim, label_size, padding_idx, dropout, num_layers)\n",
        "# optimizer = optim.Adam(model.parameters(), lr=0.0005)\n",
        "# optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "# optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=5e-4)\n",
        "# scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
        "# criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=5e-4)\n",
        "scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr=1e-5, max_lr=1e-3, step_size_up=2000, mode='triangular')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 495
        },
        "id": "4igULcZIIoUq",
        "outputId": "0a6ec494-3f50-436c-de1c-855719fdcdb8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Epoch 1, Train Loss: 1.092891667316209, Train Accuracy: 34.71441947565543\n",
            " Epoch 2, Train Loss: 1.0542176201272366, Train Accuracy: 42.25187265917603\n",
            " Epoch 3, Train Loss: 1.047173494278495, Train Accuracy: 42.25187265917603\n",
            " Epoch 4, Train Loss: 1.047044819860316, Train Accuracy: 42.25187265917603\n",
            " Epoch 5, Train Loss: 1.0475099558260903, Train Accuracy: 42.25187265917603\n",
            " Epoch 6, Train Loss: 1.0472572939609415, Train Accuracy: 42.25187265917603\n",
            " Epoch 7, Train Loss: 1.0466635827697925, Train Accuracy: 42.25187265917603\n",
            " Epoch 8, Train Loss: 1.0473447717837434, Train Accuracy: 42.25187265917603\n",
            " Epoch 9, Train Loss: 1.0465661064902347, Train Accuracy: 42.25187265917603\n",
            " Epoch 10, Train Loss: 1.0468671788920219, Train Accuracy: 42.25187265917603\n",
            " Epoch 11, Train Loss: 1.0468159144494071, Train Accuracy: 42.25187265917603\n",
            " Epoch 12, Train Loss: 1.0468325152325986, Train Accuracy: 42.25187265917603\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-da015973f147>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-9-030385973e30>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, train_iter, optimizer, criterion, num_epochs)\u001b[0m\n\u001b[1;32m     11\u001b[0m       \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m       \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m       \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m       \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m       \u001b[0mepoch_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    523\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m             )\n\u001b[0;32m--> 525\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    526\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 267\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    268\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    743\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 744\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    745\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    746\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "train_model(model,train_iter, optimizer, criterion, num_epochs =20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AfqOxjiPIoW4",
        "outputId": "f55095d5-0d5e-4667-d254-3b43475da6d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss: 1.058799535036087,  Validation Accuracy: 40.32697547683924\n"
          ]
        }
      ],
      "source": [
        "# Evaluate the model on validation data\n",
        "eval_model(model, val_iter, criterion)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QX6K0JZkI9KL",
        "outputId": "30e5a81d-4b22-4ab3-d3eb-8c3aef5b9c29"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 1.0383861133030483,  Test Accuracy: 41.13122171945702\n"
          ]
        }
      ],
      "source": [
        "# Evaluate the model on test data\n",
        "test_model (model, test_iter, criterion)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k70sF3r4YKI7",
        "outputId": "48eef6c6-6a1d-4bba-f30a-8b24b9130152"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<torchtext.data.example.Example at 0x7cf08e8830d0>"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_data[43]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RTVWywNN1RHs",
        "outputId": "e0d815cc-5bda-4b92-f112-fc1bb82705af"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predicted: negative\n",
            "Actual: negative\n",
            "Loss: 0.001959072658792138\n",
            "Correct: True\n"
          ]
        }
      ],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "def get_specific_data_point(data, index):\n",
        "    return data[index]\n",
        "\n",
        "# Evaluate a specific data point\n",
        "def evaluate_specific_data_point(model, data_point, text_field, label_field, criterion):\n",
        "    model.eval()\n",
        "\n",
        "    # Convert the data point to a tensor\n",
        "    text = text_field.process([data_point.text]).to(device)\n",
        "    label = torch.tensor([label_field.vocab.stoi[data_point.label]]).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output = model(text)\n",
        "        loss = criterion(output, label)\n",
        "\n",
        "        _, predicted = torch.max(output.data, 1)\n",
        "        is_correct = (predicted.item() == label.item())\n",
        "\n",
        "        print(f'Predicted: {label_field.vocab.itos[predicted.item()]}')\n",
        "        print(f'Actual: {data_point.label}')\n",
        "        print(f'Loss: {loss.item()}')\n",
        "        print(f'Correct: {is_correct}')\n",
        "\n",
        "# Get the specific data point\n",
        "specific_data_point = get_specific_data_point(test_data, 2000)\n",
        "\n",
        "# Evaluate the specific data point\n",
        "evaluate_specific_data_point(model, specific_data_point, TEXT, LABEL, criterion)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IoaiI-IW1V44"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kHcZIe8nIf60"
      },
      "source": [
        "Used the following hyperparameters to increase the accuracy:\n",
        "\n",
        "1. Changed the embedding_dim and hidden_dim to 256 (dense vector representation of words) check if this simple change in hyperparameters could change the accuracy. However, test accuracy @ 56% was little less than my baseline model accuracy @ 58%\n",
        "2. With the addition of item#1 added the dropout regularization technique @0.3 to see if this can help increasing the accuracy. Doing this reduce the training accuracy to 83% and improved the test accuracyt to 57%, however, this is less than my baseline model accuracy.\n",
        "3. Now adding the LSTM to 2 layers, Increasing the learning rate to 0.0001 and removing the dropout all together to see if this will increase the acccuracy - Performance is not better than item#2 infact the test accuracy dropped to 52%.\n",
        "4. Now adding the LSTM to 2 layers, Increasing the learning rate to 0.0001 and adding the dropout (0.3) to see if this will increase the acccuracy - Apparently, there was no change in test accuracy. Reached test accuracy @ 51%.\n",
        "5. Updated the embedding dimension to 512 and hidden layers to 512, batch_size=128 and dropout =0.3 and lr=0.0001 - Still testing accuracy is not better than baseline model. Poor accuracy 53%\n",
        "6. Updating the batchsize =64 and increasing number of epochs =20 and leaving the same hyperparameters as item#5 other than batchsize and epochs for this attempt as wel - didn't perform any better - test accuracy is 53%"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bdudyr6QJ2rB"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP9plyBMo/++VQzDNrayTsG",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/venkatanadikatla/pytorch/blob/main/RNN_with_SST_Better_Accuracy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "clpLVXLlIeYz",
        "outputId": "c46d3001-52aa-43a4-d1d7-704840fa8bbe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: torchtext 0.18.0\n",
            "Uninstalling torchtext-0.18.0:\n",
            "  Successfully uninstalled torchtext-0.18.0\n",
            "Collecting torchtext==0.6.0\n",
            "  Downloading torchtext-0.6.0-py3-none-any.whl (64 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.2/64.2 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torchtext==0.6.0) (4.66.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchtext==0.6.0) (2.31.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from torchtext==0.6.0) (2.3.1+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchtext==0.6.0) (1.25.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from torchtext==0.6.0) (1.16.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from torchtext==0.6.0) (0.1.99)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.6.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.6.0) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.6.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.6.0) (2024.7.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6.0) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6.0) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6.0) (1.13.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6.0) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6.0) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6.0) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch->torchtext==0.6.0)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch->torchtext==0.6.0)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch->torchtext==0.6.0)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch->torchtext==0.6.0)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch->torchtext==0.6.0)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch->torchtext==0.6.0)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch->torchtext==0.6.0)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch->torchtext==0.6.0)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch->torchtext==0.6.0)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch->torchtext==0.6.0)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch->torchtext==0.6.0)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6.0) (2.3.1)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch->torchtext==0.6.0)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m41.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->torchtext==0.6.0) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->torchtext==0.6.0) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torchtext\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.82 nvidia-nvtx-cu12-12.1.105 torchtext-0.6.0\n"
          ]
        }
      ],
      "source": [
        "!pip uninstall -y torchtext\n",
        "!pip install torchtext==0.6.0"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Project Overview**\n",
        "\n",
        "This project aims to develop a sentiment analysis model using a Bidirectionl Recurrent Neural Network (RNN) with Long Short-Term Memory (LSTM) units. The model is trained on the Stanford Sentiment Treebank (SST) dataset, which consists of sentences labeled with sentiment categories (positive or negative or neutral). The objective is to classify the sentiment of sentences accurately.\n",
        "\n",
        "Hyperparameters: **Please see the last cell of this ipynb file to see the hyperparameters change and the corresponding accuracy.**\n",
        "\n",
        "Defining the RNN model: Experimented with various combinations of bidirectional LSTM, dropout, and optimizers like Adam, SGD, and RMSprop.\n",
        "\n",
        "Firstly embedding that coverts words into dense vectors, BRNN lstm - this layer proccesses the embedded text sequences, fully connected layer - this to outputs the final predictions. Initiating a zerostate fuction to initialize the hidden state with zeros. Initiating the forward function to forward pass of the model.\n",
        "\n",
        "Training the model: This training function iterates over the training data, performs forward and backward passes, updates the model parameters, and computes the training loss and accuracy.\n",
        "\n",
        "Our goal was to improve the testing accuracy of the sentiment analysis model beyond the baseline model's performance. I've experimented with various hyperparameters to achieve the **test accuracy of 62.8%**\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "cUx7ncQ-Byoh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cibFuZirIoEF",
        "outputId": "50aa09d7-f31a-462e-c926-82f549ec93ab",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "downloading trainDevTestTrees_PTB.zip\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "trainDevTestTrees_PTB.zip: 100%|██████████| 790k/790k [00:00<00:00, 3.65MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "extracting\n"
          ]
        }
      ],
      "source": [
        "import copy # this module provides functions to duplicate objects. It seems to be imported but not yet used in the code\n",
        "import torch # The MAIN PyTorch package\n",
        "from torch import nn # contains the essential modules for building NN in pytorch.\n",
        "from torch import optim # Provides optimization algorithms, such as SGD, Adam, etc\n",
        "import torchtext # A library for text processing that works well with pytorch (Currently a version 0.6.0 is being used in this code)\n",
        "from torchtext import data # A module in torchtext used for data handling\n",
        "from torchtext import datasets # provides datasets, including various NLP datasets.\n",
        "\n",
        "TEXT = data.Field(sequential=True, batch_first=True, lower=True) # Sequential = True indicates that the data consists of sequences.\n",
        "#Batch_first=True Ensure that batch dimenstion is the first dimension in the tensor. # lower=True Converts all the text to lowercase\n",
        "\n",
        "LABEL = data.LabelField() # A subclass of Field specifically for handling labels in a classification task.\n",
        "\n",
        "# load data splits\n",
        "train_data, val_data, test_data = datasets.SST.splits(TEXT, LABEL) #datasets.SST.splits - Loads the Standford Sentiment Treebank(SST) dataset and splits the dataset\n",
        "\n",
        "# build dictionary\n",
        "# build_vocab: Creates a mapping from tokens(words) to indices. This is essential for converting text data into numerical form that can be used by NN.\n",
        "TEXT.build_vocab(train_data) # Builds the vocabulary for the text field using the training data.\n",
        "LABEL.build_vocab(train_data)# Builds the vocabulary for the label field using the training data.\n",
        "\n",
        "# hyperparameters\n",
        "vocab_size = len(TEXT.vocab) # the size of the vocabulary (number of unique tokens in the training data)\n",
        "label_size = len(LABEL.vocab) # the number of unique labels (classes) in the traning data\n",
        "padding_idx = TEXT.vocab.stoi['<pad>'] # The index used for padding sequences to the same length\n",
        "embedding_dim = 300  # The size of the word embeddings (dense vector representation of words)\n",
        "hidden_dim = 256 # Size of the hidden layers in the model\n",
        "dropout = 0.5\n",
        "num_layers = 2\n",
        "# patience =3\n",
        "\n",
        "# build iterators\n",
        "train_iter, val_iter, test_iter = data.BucketIterator.splits(\n",
        "    (train_data, val_data, test_data),\n",
        "    batch_size=64)\n",
        "\n",
        "# Data.bucketiterator.splits - Creates iterators for the training, validation and test sets.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Print a few examples from the raw training data\n",
        "# for i in range(50):\n",
        "#     print(vars(train_data.examples[i]))\n",
        "# import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "nZuIUd-AoecG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Adding attention mechanism here to help the model focus on different part of the input sequence when making predictions.\n",
        "# class Attention(nn.Module):\n",
        "#     def __init__(self, hidden_dim):\n",
        "#         super(Attention, self).__init__()\n",
        "#         self.hidden_dim = hidden_dim\n",
        "#         self.attn = nn.Linear(hidden_dim * 2, hidden_dim)\n",
        "#         self.v = nn.Parameter(torch.rand(hidden_dim))\n",
        "\n",
        "#     def forward(self, hidden, encoder_outputs):\n",
        "#         timestep = encoder_outputs.size(1)\n",
        "#         # h = hidden.repeat(timestep, 1, 1).transpose(0, 1)\n",
        "#         h = hidden.unsqueeze(1).repeat(1, timestep, 1)\n",
        "#         print(f'h shape: {h.shape}')\n",
        "#         attn_energies = self.score(h, encoder_outputs)\n",
        "#         return F.functional.softmax(attn_energies, dim=1)\n",
        "\n",
        "#     def score(self, hidden, encoder_outputs):\n",
        "#         energy = torch.tanh(self.attn(torch.cat([hidden, encoder_outputs], 2)))\n",
        "#         energy = energy.transpose(2, 1)\n",
        "#         v = self.v.repeat(encoder_outputs.size(0), 1).unsqueeze(1)\n",
        "#         energy = torch.bmm(v, energy)\n",
        "#         return energy.squeeze(1)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "3Z75aG8Coupi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DCtbhUqnIoGT"
      },
      "outputs": [],
      "source": [
        "class RNN (torch.nn.Module):\n",
        "  def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, padding_idx,dropout, num_layers=2):\n",
        "    super().__init__()\n",
        "    self.vocab_size = vocab_size\n",
        "    self.embedding_dim = embedding_dim\n",
        "    self.hidden_dim = hidden_dim\n",
        "    self.label_size = label_size\n",
        "    self.num_layers = num_layers\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    self.padding_idx = padding_idx\n",
        "    self.embedding = torch.nn.Embedding(vocab_size, embedding_dim,padding_idx = padding_idx)\n",
        "    # self.rnn = torch.nn.RNN(embedding_dim,hidden_dim, nonlinearity='relu',batch_first=True)\n",
        "    self.lstm = torch.nn.LSTM(embedding_dim,hidden_dim, num_layers=num_layers,dropout=dropout, batch_first=True, bidirectional=True)\n",
        "    # self.attention = Attention(hidden_dim)\n",
        "    self.fc = torch.nn.Linear(hidden_dim*2, output_dim) #hidden *2 because of BRNN\n",
        "\n",
        "\n",
        "  def zero_state(self, batch_size):\n",
        "    # Implement the function, which returns an initial hidden state.\n",
        "    return (torch.zeros(self.num_layers*2, batch_size, self.hidden_dim),\n",
        "            torch.zeros(self.num_layers*2, batch_size, self.hidden_dim))\n",
        "\n",
        "  def forward(self, text):\n",
        "    embedded = self.embedding(text)\n",
        "    embedded = self.dropout(embedded)\n",
        "    batch_size = text.size(0)\n",
        "    h_0, c_0 = self.zero_state(batch_size)\n",
        "    h_0, c_0 = h_0.to(text.device), c_0.to(text.device)  # Ensure the hidden state is on the same device as the input\n",
        "    output, (hidden,cell) = self.lstm(embedded, (h_0, c_0))\n",
        "    # Check dimensions\n",
        "    # print(f'lstm_output shape: {output.shape}')  # [batch_size, seq_len, hidden_dim * 2]\n",
        "    # print(f'hidden shape: {hidden.shape}')  # [num_layers * 2, batch_size, hidden_dim]\n",
        "\n",
        "\n",
        "    # hidden = hidden[-1]\n",
        "    hidden = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1)\n",
        "    # print(f'concatenated hidden shape: {hidden.shape}')  # [batch_size, hidden_dim * 2]\n",
        "    # attn_weights = self.attention(hidden[-1], output)\n",
        "    # context = attn_weights.unsqueeze(1).bmm(output).squeeze(1)\n",
        "    # Check dimensions\n",
        "    # print(f'context shape: {context.shape}')  # [batch_size, hidden_dim * 2]\n",
        "\n",
        "    return self.fc(self.dropout(hidden))\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d6ce0_3BIoI1"
      },
      "outputs": [],
      "source": [
        "def train_model(model,train_iter, optimizer, criterion, num_epochs =20):\n",
        "  model.train()\n",
        "  # best_val_loss = float('inf')\n",
        "  # patience_counter = 0\n",
        "  for epoch in range(num_epochs):\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for batch in train_iter:\n",
        "      optimizer.zero_grad()\n",
        "      text, labels = batch.text, batch.label\n",
        "      output = model(text)\n",
        "      loss = criterion(output, labels)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      epoch_loss += loss.item()\n",
        "      epoch_acc += (output.argmax(1) ==labels).sum().item()\n",
        "\n",
        "      _, predicted = torch.max(output.data,1)\n",
        "      total += labels.size(0)\n",
        "      correct += (predicted == labels).sum().item()\n",
        "\n",
        "    avg_loss = epoch_loss/len(train_iter)\n",
        "    avg_acc = epoch_acc/len(train_iter.dataset)\n",
        "\n",
        "    epoch_accuracy = 100*correct/total\n",
        "\n",
        "\n",
        "    print(f' Epoch {epoch+1}, Train Loss: {avg_loss}, Train Accuracy: {epoch_accuracy}')\n",
        "\n",
        "    #Early Stopping\n",
        "    # val_loss = eval_model(model, val_iter, criterion)\n",
        "    # if val_loss < best_val_loss:\n",
        "    #   best_val_loss = val_loss\n",
        "    #   patience_counter = 0\n",
        "\n",
        "    # else:\n",
        "    #   patience_counter +=1\n",
        "\n",
        "    # if patience_counter >= patience:\n",
        "    #   print('Early Stopping')\n",
        "    #   break\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G-1jLhCCIoK3"
      },
      "outputs": [],
      "source": [
        "def eval_model(model, val_iter, criterion):\n",
        "  model.eval()\n",
        "  epoch_loss = 0\n",
        "  epoch_acc = 0\n",
        "  correct = 0\n",
        "  total = 0\n",
        "  for batch in val_iter:\n",
        "    text, labels = batch.text, batch.label\n",
        "    output = model(text)\n",
        "    loss = criterion(output, labels)\n",
        "    epoch_loss +=loss.item()\n",
        "    epoch_acc += (output.argmax(1)==labels).sum().item()\n",
        "    _, predicted = torch.max(output.data,1)\n",
        "    total += labels.size(0)\n",
        "    correct += (predicted == labels).sum().item()\n",
        "\n",
        "  avg_loss = epoch_loss/len(val_iter)\n",
        "  avg_acc = epoch_acc/len(val_iter.dataset)\n",
        "\n",
        "  epoch_accuracy = 100*avg_acc\n",
        "\n",
        "  print(f'Validation Loss: {avg_loss},  Validation Accuracy: {epoch_accuracy}')\n",
        "  return avg_loss\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lJlXVgETIoPW"
      },
      "outputs": [],
      "source": [
        "def test_model(model, test_iter, criterion):\n",
        "  model.eval()\n",
        "  epoch_loss = 0\n",
        "  epoch_acc = 0\n",
        "  correct = 0\n",
        "  total = 0\n",
        "  for batch in test_iter:\n",
        "    text, labels = batch.text, batch.label\n",
        "    output = model(text)\n",
        "    loss = criterion(output, labels)\n",
        "    epoch_loss +=loss.item()\n",
        "    epoch_acc += (output.argmax(1)==labels).sum().item()\n",
        "    _, predicted = torch.max(output.data,1)\n",
        "    total += labels.size(0)\n",
        "    correct += (predicted == labels).sum().item()\n",
        "\n",
        "  avg_loss = epoch_loss/len(test_iter)\n",
        "  avg_acc = epoch_acc/len(test_iter.dataset)\n",
        "\n",
        "  epoch_accuracy = 100*correct/total\n",
        "\n",
        "  print(f'Test Loss: {avg_loss},  Test Accuracy: {epoch_accuracy}')\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2udT32qiIoR1"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(42)\n",
        "# model = RNN(vocab_size,embedding_dim,hidden_dim,label_size, padding_idx, )\n",
        "model = RNN(vocab_size, embedding_dim, hidden_dim, label_size, padding_idx,dropout, num_layers)\n",
        "# optimizer = optim.Adam(model.parameters(), lr=0.0005)\n",
        "# optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "# optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=5e-4)\n",
        "# scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
        "# optimizer = optim.RMSprop(model.parameters(), lr=0.0001, alpha=0.9, weight_decay=1e-4)\n",
        "# scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
        "# optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
        "# optimizer = optim.Adagrad(model.parameters(), lr=0.01)\n",
        "# optimizer = optim.Adadelta(model.parameters(), lr=1.0, rho=0.9)\n",
        "# optimizer = optim.Adamax(model.parameters(), lr=0.002)\n",
        "# optimizer = optim.NAdam(model.parameters(), lr=0.002)\n",
        "optimizer = optim.RMSprop(model.parameters(), lr=0.001, alpha=0.9)\n",
        "\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# criterion = nn.CrossEntropyLoss()\n",
        "# optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=5e-4)\n",
        "# scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr=1e-5, max_lr=1e-3, step_size_up=2000, mode='triangular')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4igULcZIIoUq",
        "outputId": "aa1b017d-6e60-417a-8787-b669dbe3ccc5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Epoch 1, Train Loss: 1.0358599170820038, Train Accuracy: 46.125936329588015\n",
            " Epoch 2, Train Loss: 0.9945817313977142, Train Accuracy: 51.767322097378276\n",
            " Epoch 3, Train Loss: 0.9506405409592301, Train Accuracy: 56.08614232209738\n",
            " Epoch 4, Train Loss: 0.9000712179425937, Train Accuracy: 59.02387640449438\n",
            " Epoch 5, Train Loss: 0.8515530215270484, Train Accuracy: 61.96161048689139\n",
            " Epoch 6, Train Loss: 0.7979917757546724, Train Accuracy: 65.78885767790263\n",
            " Epoch 7, Train Loss: 0.7494230328211143, Train Accuracy: 67.39232209737828\n",
            " Epoch 8, Train Loss: 0.7062678027953675, Train Accuracy: 70.02574906367042\n",
            " Epoch 9, Train Loss: 0.6680171409649636, Train Accuracy: 71.80477528089888\n",
            " Epoch 10, Train Loss: 0.6300494319467402, Train Accuracy: 73.87640449438203\n",
            " Epoch 11, Train Loss: 0.5619215324743471, Train Accuracy: 76.79073033707866\n",
            " Epoch 12, Train Loss: 0.5329100060373989, Train Accuracy: 78.34737827715355\n",
            " Epoch 13, Train Loss: 0.4928752982349538, Train Accuracy: 80.10299625468164\n",
            " Epoch 14, Train Loss: 0.45560585640704454, Train Accuracy: 81.74157303370787\n",
            " Epoch 15, Train Loss: 0.4340403056634006, Train Accuracy: 82.64279026217228\n",
            " Epoch 16, Train Loss: 0.39399628218875005, Train Accuracy: 84.58567415730337\n",
            " Epoch 17, Train Loss: 0.3615207426361184, Train Accuracy: 86.00187265917603\n",
            " Epoch 18, Train Loss: 0.3415996416736005, Train Accuracy: 86.96161048689139\n",
            " Epoch 19, Train Loss: 0.32793799220626035, Train Accuracy: 87.40636704119851\n",
            " Epoch 20, Train Loss: 0.3079912821525958, Train Accuracy: 88.34269662921348\n"
          ]
        }
      ],
      "source": [
        "train_model(model,train_iter, optimizer, criterion, num_epochs =20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AfqOxjiPIoW4",
        "outputId": "c32aae6d-a09b-413f-9028-3b66337aaef6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss: 2.2240900529755487,  Validation Accuracy: 58.31062670299727\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2.2240900529755487"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "# Evaluate the model on validation data\n",
        "eval_model(model, val_iter, criterion)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QX6K0JZkI9KL",
        "outputId": "e7a42c3a-c384-4237-9a6f-bfe59e9306ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 1.9391816888536726,  Test Accuracy: 62.66968325791855\n"
          ]
        }
      ],
      "source": [
        "# Evaluate the model on test data\n",
        "test_model (model, test_iter, criterion)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k70sF3r4YKI7",
        "outputId": "f36fedcc-237b-4233-de43-8135feca74ed"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torchtext.data.example.Example at 0x79aadbe9f7f0>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "test_data[43]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RTVWywNN1RHs",
        "outputId": "6f5ad5ba-cc63-4e4d-c4c6-07b5651549c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted: negative\n",
            "Actual: negative\n",
            "Loss: 5.722029527532868e-06\n",
            "Correct: True\n"
          ]
        }
      ],
      "source": [
        "#This function to show the test or anyunown data to check the quick check:\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "def get_specific_data_point(data, index):\n",
        "    return data[index]\n",
        "\n",
        "# Evaluate a specific data point\n",
        "def evaluate_specific_data_point(model, data_point, text_field, label_field, criterion):\n",
        "    model.eval()\n",
        "\n",
        "    # Convert the data point to a tensor\n",
        "    text = text_field.process([data_point.text]).to(device)\n",
        "    label = torch.tensor([label_field.vocab.stoi[data_point.label]]).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output = model(text)\n",
        "        loss = criterion(output, label)\n",
        "\n",
        "        _, predicted = torch.max(output.data, 1)\n",
        "        is_correct = (predicted.item() == label.item())\n",
        "\n",
        "        print(f'Predicted: {label_field.vocab.itos[predicted.item()]}')\n",
        "        print(f'Actual: {data_point.label}')\n",
        "        print(f'Loss: {loss.item()}')\n",
        "        print(f'Correct: {is_correct}')\n",
        "\n",
        "# Get the specific data point\n",
        "specific_data_point = get_specific_data_point(test_data, 2000)\n",
        "\n",
        "# Evaluate the specific data point\n",
        "evaluate_specific_data_point(model, specific_data_point, TEXT, LABEL, criterion)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kHcZIe8nIf60"
      },
      "source": [
        "Used the following hyperparameters to increase the accuracy:\n",
        "\n",
        "1. Changed the embedding_dim and hidden_dim to 256 (dense vector representation of words) check if this simple change in hyperparameters could change the accuracy. **However, test accuracy @ 56% was close to my baseline model accuracy @ 56.6%**\n",
        "2. With the addition of item#1 added the dropout regularization technique @ 0.3 to see if this can help increasing the accuracy. **Improved the test accuracyt to 57%, however, this is slightly better than my baseline model accuracy.**\n",
        "3. Now adding the LSTM to 2 layers, Increasing the learning rate to 0.0001 and removing the dropout all together to see if this will increase the acccuracy - **Performance is not better than item#2 infact the test accuracy dropped to 52%.**\n",
        "4. Now adding the LSTM to 2 layers, Increasing the learning rate to 0.0001 and adding the dropout (0.3) to see if this will increase the acccuracy - **Apparently, there was no change in test accuracy. Reached test accuracy @ 51%.**\n",
        "5. Updated the embedding dimension to 512 and hidden layers to 512, batch_size=128 and dropout =0.3 and lr=0.0001 - Still testing accuracy is not better than baseline model. **Performance is not better than baseline performance infact the test accuracy dropped to 53%**\n",
        "6. Updating the batchsize =64 and increasing number of epochs =20 and leaving the same hyperparameters as item#5 other than batchsize and epochs for this attempt as wel - **didn't perform any better - test accuracy is 53%**\n",
        "7. Used **BRNN LSTM and dropout rate and Adam optimizer weight decay** - didn't help the model test performance, reached only **51% accuracy.**\n",
        "8. Used **BRNN LSTM and dropout ratge with SGD(optim.SGD(model.parameters(), lr=0.001, momentum=0.9)) - poor model test accuracy @ 42%**\n",
        "9. Used BRNN LSTM and dropout rate 0.5 with optim.RMSprop(model.parameters(), lr=0.001, alpha=0.9) - **poor model test accuracy @ 57% but slightly less than baseline model accuracy.**\n",
        "10. Used BRNN LSTM and dropout rate 0.5 with optim.RMSprop(model.parameters(), lr=0.0001, alpha=0.9), reduced the hidden_layers to 256 and raised epochs to 20 - ***Got the better accuracy than the base line model - Test accuracy 59%***\n",
        "11. Used BRNN LSTM, removed dropout with optim.RMSprop(model.parameters(), lr=0.0001, alpha=0.9), reduced the hidden_layers to 256 and raised epochs to 20 - ***Got the better accuracy than the base line model - Test accuracy 59.5%***\n",
        "12. Used BRNN LSTM, removed dropout with optim.RMSprop(model.parameters(), lr=0.0001, alpha=0.9, weight_decay=1e-4), reduced the hidden_layers to 256 and raised epochs to 20 - ***Got the poor accuracy than the base line model - Test accuracy 53.5%***\n",
        "13. Used BRNN LSTM, added dropout with optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-4), reduced the hidden_layers to 256 and raised epochs to 20 - ***Got the better accuracy than base line model - test accuracy 58.9%***\n",
        "14. Used BRNN LSTM, added dropout with optimizer = optim.Adagrad(model.parameters(), lr=0.01), reduced the hidden_layers to 256 and raised epochs to 20 - ***Got the  accuracy slightly better than base line model - test accuracy of 57%***\n",
        "15. Used BRNN LSTM, added dropout with optimizer = optim.Adadelta(model.parameters(), lr=1.0, rho=0.9) , reduced the hidden_layers to 256 and raised epochs to 20 - ***Got the  better accuracy than base line model - test accuracy of 58.2%***\n",
        "16. Used BRNN LSTM, added dropout with optimizer = optim.Adamax(model.parameters(), lr=0.002) , reduced the hidden_layers to 256 and raised epochs to 20 - ***Got the accuracy same as base line model - test accuracy of 56.7%***\n",
        "17.  Used BRNN LSTM, added dropout with optimizer = optim.NAdam(model.parameters(), lr=0.002) , reduced the hidden_layers to 256 and reduced epochs to 10 - ***Got the accuracy slightly better than base line model - test accuracy of 57.8%***\n",
        "18. Used BRNN LSTM, added dropout with optim.RMSprop(model.parameters(), lr=0.0001, alpha=0.9, weight_decay=1e-4) and in addition added early stoping regularization technique , reduced the hidden_layers to 256 and  epochs to 20 - ***Got the accuracy slightly better than base line model - test accuracy of 57.2%***\n",
        "19. Used BRNN LSTM, added dropout with optim.RMSprop(model.parameters(), lr=0.001, alpha=0.9) and in addition removed early stoping regularization technique , reduced the hidden_layers to 256 and maintained epochs to 20 - ***Got the accuracy better than base line model - test accuracy of 62.6%***\n",
        "20. Used BRNN LSTM, added dropout with optim.RMSprop(model.parameters(), lr=0.001, alpha=0.9) and in addition removed early stoping regularization technique , increased word_embedding to 512, maintained the hidden_layers to 256 and maintained epochs to 20 - ***Got the accuracy better than base line model - test accuracy of 60.8%***\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**BRNN with LSTM and Different Optimizers:** Experimented with various combinations of bidirectional LSTM, dropout, and optimizers like Adam, SGD, and RMSprop.\n",
        "\n",
        "**Best Configuration:** **BRNN with LSTM, dropout rate 0.5, RMSprop optimizer (lr=0.001, alpha=0.9). Achieved the highest test accuracy of 62.6%.**\n",
        "\n",
        "\n",
        "**Conclusion:**\n",
        "Throughout the project, various hyperparameters and configurations were experimented with to enhance the model's test accuracy beyond the baseline. The use of bidirectional LSTM, dropout regularization, and RMSprop optimizer proved to be the most effective combination, achieving a test accuracy of 62.6%. This improvement demonstrates the potential of fine-tuning hyperparameters to enhance model performance, making sentiment analysis more accurate and reliable for business applications."
      ],
      "metadata": {
        "id": "HraybBn4JGHg"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QUefhPaxJRuG"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPFkfzyc9Asp/Oukv2fN2bV",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPrj73v/ckLvLL7ik/6s4UN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "ir",
      "display_name": "R"
    },
    "language_info": {
      "name": "R"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/venkatanadikatla/pytorch/blob/main/Hierarchical_Normal_Model_%26_Gibbs_Sampling_R.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data Overview:**\n",
        "The dataset consists of annual death counts from five causes for the age group 25–34 over five years (2018–2022). These causes are:\n",
        "\n",
        "Accidents, Homicide, Suicide, Heart Disease, Malignant Neoplasms\n",
        "The goal is to model these death counts using three approaches:\n",
        "\n",
        "\n",
        "*   Separate Gaussian model\n",
        "\n",
        "*   Pooled Gaussian model\n",
        "*   Hierarchical Gaussian model\n",
        "\n",
        "**Why Gibbs Sampling:**\n",
        "Gibbs sampling is a special case of the Metropolis-Hastings algorithm used for Bayesian inference, particularly when the full conditional distributions of the variables in a model can be easily sampled.\n",
        "\n",
        "**a. Separate Model:** In a separate model, we treat each group or data source independently. When using Gibbs sampling here, each group's parameters are sampled independently from their respective conditional distributions.\n",
        "\n",
        "\n",
        "\n",
        "*   **Specific to the model:**  Sample the parameters for each cause of death independently. Each death cause has its own parameters, which could lead to high variability in small samples.\n",
        "\n",
        "\n",
        "**b. Pooled Model:** In a pooled model, the data from different groups are combined (or \"pooled\") and treated as if they came from a single distribution. Gibbs sampling in this model would sample the parameters from a single conditional distribution across all data. This reduces variance but may miss group-specific effects, assuming all groups are homogenous.\n",
        "\n",
        "*   **Specific to the model:**  Pooled Model (Gibbs): Sample from a common parameter for all causes, assuming homogeneity.\n",
        "\n",
        "**c. Hierarchical Model** (or Partially Pooled Model): In hierarchical models, group-level parameters (for individual groups) are treated as random variables drawn from a higher-level (hyperparameter) distribution. Gibbs sampling here involves sampling both the group-level parameters and the hyperparameters in a conditional step-by-step process. This approach balances between separate and pooled models, allowing group-specific variability while sharing information across groups.\n",
        "\n",
        "*   **Specific to the model:** Sample group-specific parameters and hyperparameters, allowing for shared information while maintaining some independence between causes.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "htIVRYa-bLJz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "2QWzYA2OZjZ0",
        "outputId": "d99c05ff-53af-4859-cc6b-64e8f02b936c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "r-base is already the newest version (4.4.1-3.2204.0).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 49 not upgraded.\n"
          ]
        }
      ],
      "source": [
        "!apt-get install -y r-base\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install the IRkernel package for R support\n",
        "!R -e \"IRkernel::installspec(user = FALSE)\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "5xDHGV1XlkgV",
        "outputId": "492ae828-9225-4fce-ce6c-ac087048a779"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "R version 4.4.1 (2024-06-14) -- \"Race for Your Life\"\n",
            "Copyright (C) 2024 The R Foundation for Statistical Computing\n",
            "Platform: x86_64-pc-linux-gnu\n",
            "\n",
            "R is free software and comes with ABSOLUTELY NO WARRANTY.\n",
            "You are welcome to redistribute it under certain conditions.\n",
            "Type 'license()' or 'licence()' for distribution details.\n",
            "\n",
            "  Natural language support but running in an English locale\n",
            "\n",
            "R is a collaborative project with many contributors.\n",
            "Type 'contributors()' for more information and\n",
            "'citation()' on how to cite R or R packages in publications.\n",
            "\n",
            "Type 'demo()' for some demos, 'help()' for on-line help, or\n",
            "'help.start()' for an HTML browser interface to help.\n",
            "Type 'q()' to quit R.\n",
            "\n",
            "> IRkernel::installspec(user = FALSE)\n",
            "\u001b[?25h> \n",
            "> \n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install dependencies and RStan\n",
        "install.packages(\"rstan\", repos = \"https://cloud.r-project.org/\", dependencies=TRUE)\n",
        "\n",
        "# Load the library\n",
        "library(rstan)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "ru3LMKIylk8I",
        "outputId": "ad76cf0c-cff2-4e98-efbd-1f36f073f7e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Installing package into ‘/usr/local/lib/R/site-library’\n",
            "(as ‘lib’ is unspecified)\n",
            "\n",
            "also installing the dependencies ‘numDeriv’, ‘lazyeval’, ‘abind’, ‘tensorA’, ‘distributional’, ‘crosstalk’, ‘zoo’, ‘plyr’, ‘igraph’, ‘checkmate’, ‘matrixStats’, ‘posterior’, ‘colourpicker’, ‘DT’, ‘dygraphs’, ‘gtools’, ‘markdown’, ‘reshape2’, ‘shinyjs’, ‘shinythemes’, ‘threejs’, ‘xts’, ‘ggridges’, ‘StanHeaders’, ‘inline’, ‘gridExtra’, ‘RcppParallel’, ‘loo’, ‘QuickJSR’, ‘RcppEigen’, ‘BH’, ‘shinystan’, ‘bayesplot’, ‘rstantools’, ‘coda’, ‘V8’\n",
            "\n",
            "\n",
            "Loading required package: StanHeaders\n",
            "\n",
            "\n",
            "rstan version 2.32.6 (Stan version 2.32.2)\n",
            "\n",
            "\n",
            "For execution on a local, multicore CPU with excess RAM we recommend calling\n",
            "options(mc.cores = parallel::detectCores()).\n",
            "To avoid recompilation of unchanged Stan programs, we recommend calling\n",
            "rstan_options(auto_write = TRUE)\n",
            "For within-chain threading using `reduce_sum()` or `map_rect()` Stan functions,\n",
            "change `threads_per_chain` option:\n",
            "rstan_options(threads_per_chain = 1)\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**General Setup:**"
      ],
      "metadata": {
        "id": "h6mZm_uvxnz4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Death data for the five causes from 2018 to 2022\n",
        "deaths <- matrix(c(33058, 34452, 31315, 24516, 24614,   # Accidents\n",
        "                   6712, 7571, 31315, 5341, 5234,       # Homicide\n",
        "                   8663, 8862, 8454, 8059, 8020,       # Suicide\n",
        "                   3789, 4155, 3984, 3495, 3561,       # Heart Disease\n",
        "                   3641, 3615, 3573, 3577, 3684),      # Malignant Neoplasms\n",
        "                 ncol = 5, byrow = TRUE)\n",
        "\n",
        "years <- c(2018, 2019, 2020, 2021, 2022)\n",
        "n_years <- length(years)\n",
        "n_causes <- 5\n"
      ],
      "metadata": {
        "id": "VkSRFaVwqAkl"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Separate Gaussian Model:** Each cause has its own mean and shared variance"
      ],
      "metadata": {
        "id": "PiFHsp-eycE2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "separate_model_code <- \"\n",
        "data {\n",
        "  int<lower=0> N;   // number of years\n",
        "  int<lower=0> C;   // number of causes\n",
        "  matrix[N, C] y;   // data for deaths\n",
        "}\n",
        "parameters {\n",
        "  vector[C] mu;     // mean for each cause\n",
        "  real<lower=0> sigma;   // common standard deviation\n",
        "}\n",
        "model {\n",
        "  for (n in 1:N) {\n",
        "    for (c in 1:C) {\n",
        "      y[n, c] ~ normal(mu[c], sigma);  // Separate normal distribution for each cause\n",
        "    }\n",
        "  }\n",
        "}\n",
        "generated quantities {\n",
        "  real mu_post[C];\n",
        "  real y_pred[C];\n",
        "\n",
        "  // Posterior mean of the causes\n",
        "  for (c in 1:C)\n",
        "    mu_post[c] = mu[c];\n",
        "\n",
        "  // Predictive distribution for a hypothetical 6th cause\n",
        "  for (c in 1:C)\n",
        "    y_pred[c] = normal_rng(mu[c], sigma);\n",
        "}\n",
        "\"\n",
        "\n",
        "\n",
        "# Prepare data for Stan\n",
        "separate_data <- list(N = n_years, C = n_causes, y = deaths)\n",
        "\n",
        "# Fit the model using Stan\n",
        "separate_fit <- stan(model_code = separate_model_code, data = separate_data,\n",
        "                     iter = 2000, chains = 4)\n",
        "\n",
        "# Extract results\n",
        "print(separate_fit)\n",
        "\n"
      ],
      "metadata": {
        "id": "bgDonGyJxwpB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e1b739a-199c-46b1-d775-c9367f3dad95"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "SAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1).\n",
            "Chain 1: \n",
            "Chain 1: Gradient evaluation took 1.2e-05 seconds\n",
            "Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.12 seconds.\n",
            "Chain 1: Adjust your expectations accordingly!\n",
            "Chain 1: \n",
            "Chain 1: \n",
            "Chain 1: Iteration:    1 / 2000 [  0%]  (Warmup)\n",
            "Chain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)\n",
            "Chain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)\n",
            "Chain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)\n",
            "Chain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)\n",
            "Chain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)\n",
            "Chain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)\n",
            "Chain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)\n",
            "Chain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)\n",
            "Chain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)\n",
            "Chain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)\n",
            "Chain 1: Iteration: 2000 / 2000 [100%]  (Sampling)\n",
            "Chain 1: \n",
            "Chain 1:  Elapsed Time: 0.442 seconds (Warm-up)\n",
            "Chain 1:                0.042 seconds (Sampling)\n",
            "Chain 1:                0.484 seconds (Total)\n",
            "Chain 1: \n",
            "\n",
            "SAMPLING FOR MODEL 'anon_model' NOW (CHAIN 2).\n",
            "Chain 2: \n",
            "Chain 2: Gradient evaluation took 8e-06 seconds\n",
            "Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.08 seconds.\n",
            "Chain 2: Adjust your expectations accordingly!\n",
            "Chain 2: \n",
            "Chain 2: \n",
            "Chain 2: Iteration:    1 / 2000 [  0%]  (Warmup)\n",
            "Chain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)\n",
            "Chain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)\n",
            "Chain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)\n",
            "Chain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)\n",
            "Chain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)\n",
            "Chain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)\n",
            "Chain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)\n",
            "Chain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)\n",
            "Chain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)\n",
            "Chain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)\n",
            "Chain 2: Iteration: 2000 / 2000 [100%]  (Sampling)\n",
            "Chain 2: \n",
            "Chain 2:  Elapsed Time: 0.424 seconds (Warm-up)\n",
            "Chain 2:                0.041 seconds (Sampling)\n",
            "Chain 2:                0.465 seconds (Total)\n",
            "Chain 2: \n",
            "\n",
            "SAMPLING FOR MODEL 'anon_model' NOW (CHAIN 3).\n",
            "Chain 3: \n",
            "Chain 3: Gradient evaluation took 9e-06 seconds\n",
            "Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.09 seconds.\n",
            "Chain 3: Adjust your expectations accordingly!\n",
            "Chain 3: \n",
            "Chain 3: \n",
            "Chain 3: Iteration:    1 / 2000 [  0%]  (Warmup)\n",
            "Chain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)\n",
            "Chain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)\n",
            "Chain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)\n",
            "Chain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)\n",
            "Chain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)\n",
            "Chain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)\n",
            "Chain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)\n",
            "Chain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)\n",
            "Chain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)\n",
            "Chain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)\n",
            "Chain 3: Iteration: 2000 / 2000 [100%]  (Sampling)\n",
            "Chain 3: \n",
            "Chain 3:  Elapsed Time: 0.389 seconds (Warm-up)\n",
            "Chain 3:                0.041 seconds (Sampling)\n",
            "Chain 3:                0.43 seconds (Total)\n",
            "Chain 3: \n",
            "\n",
            "SAMPLING FOR MODEL 'anon_model' NOW (CHAIN 4).\n",
            "Chain 4: \n",
            "Chain 4: Gradient evaluation took 8e-06 seconds\n",
            "Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.08 seconds.\n",
            "Chain 4: Adjust your expectations accordingly!\n",
            "Chain 4: \n",
            "Chain 4: \n",
            "Chain 4: Iteration:    1 / 2000 [  0%]  (Warmup)\n",
            "Chain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)\n",
            "Chain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)\n",
            "Chain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)\n",
            "Chain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)\n",
            "Chain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)\n",
            "Chain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)\n",
            "Chain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)\n",
            "Chain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)\n",
            "Chain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)\n",
            "Chain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)\n",
            "Chain 4: Iteration: 2000 / 2000 [100%]  (Sampling)\n",
            "Chain 4: \n",
            "Chain 4:  Elapsed Time: 0.424 seconds (Warm-up)\n",
            "Chain 4:                0.044 seconds (Sampling)\n",
            "Chain 4:                0.468 seconds (Total)\n",
            "Chain 4: \n",
            "Inference for Stan model: anon_model.\n",
            "4 chains, each with iter=2000; warmup=1000; thin=1; \n",
            "post-warmup draws per chain=1000, total post-warmup draws=4000.\n",
            "\n",
            "               mean se_mean       sd      2.5%      25%      50%      75%\n",
            "mu[1]      11176.61   82.69  5821.30     82.20  7410.88 11177.12 15135.11\n",
            "mu[2]      11646.62   74.86  5702.07    580.23  7892.71 11619.25 15469.36\n",
            "mu[3]      15682.72   75.66  5677.16   4291.15 11905.61 15685.11 19457.83\n",
            "mu[4]       8933.57   77.73  5735.60  -2403.58  5176.40  8899.08 12718.59\n",
            "mu[5]       8855.98   78.01  5754.10  -2203.96  5025.32  8816.46 12665.87\n",
            "sigma      12568.35   37.86  2131.68   9283.80 11044.47 12320.68 13742.19\n",
            "mu_post[1] 11176.61   82.69  5821.30     82.20  7410.88 11177.12 15135.11\n",
            "mu_post[2] 11646.62   74.86  5702.07    580.23  7892.71 11619.25 15469.36\n",
            "mu_post[3] 15682.72   75.66  5677.16   4291.15 11905.61 15685.11 19457.83\n",
            "mu_post[4]  8933.57   77.73  5735.60  -2403.58  5176.40  8899.08 12718.59\n",
            "mu_post[5]  8855.98   78.01  5754.10  -2203.96  5025.32  8816.46 12665.87\n",
            "y_pred[1]  10972.14  214.81 13785.08 -15479.86  2215.41 10989.01 19962.69\n",
            "y_pred[2]  11422.68  226.68 13742.70 -16090.96  2236.13 11440.18 20421.88\n",
            "y_pred[3]  15690.69  212.80 13820.90 -11779.17  6652.57 15609.99 24345.56\n",
            "y_pred[4]   9004.77  211.63 14005.87 -19283.68   137.59  9181.42 18248.19\n",
            "y_pred[5]   8787.20  211.88 14091.46 -18526.64  -468.48  8791.17 17872.81\n",
            "lp__        -238.12    0.05     2.00   -242.89  -239.18  -237.76  -236.64\n",
            "              97.5% n_eff Rhat\n",
            "mu[1]      22611.64  4956    1\n",
            "mu[2]      22831.41  5802    1\n",
            "mu[3]      26966.44  5630    1\n",
            "mu[4]      20243.45  5445    1\n",
            "mu[5]      20047.66  5441    1\n",
            "sigma      17633.23  3170    1\n",
            "mu_post[1] 22611.64  4956    1\n",
            "mu_post[2] 22831.41  5802    1\n",
            "mu_post[3] 26966.44  5630    1\n",
            "mu_post[4] 20243.45  5445    1\n",
            "mu_post[5] 20047.66  5441    1\n",
            "y_pred[1]  38137.94  4118    1\n",
            "y_pred[2]  37779.54  3676    1\n",
            "y_pred[3]  43518.90  4218    1\n",
            "y_pred[4]  35627.15  4380    1\n",
            "y_pred[5]  35901.11  4423    1\n",
            "lp__        -235.36  1492    1\n",
            "\n",
            "Samples were drawn using NUTS(diag_e) at Tue Sep 24 02:47:21 2024.\n",
            "For each parameter, n_eff is a crude measure of effective sample size,\n",
            "and Rhat is the potential scale reduction factor on split chains (at \n",
            "convergence, Rhat=1).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Gibbs sampling for the separate Gaussian model\n",
        "\n",
        "# Data\n",
        "deaths <- matrix(c(33058, 34452, 31315, 24516, 24614,   # Accidents\n",
        "                   6712, 7571, 31315, 5341, 5234,       # Homicide\n",
        "                   8663, 8862, 8454, 8059, 8020,       # Suicide\n",
        "                   3789, 4155, 3984, 3495, 3561,       # Heart Disease\n",
        "                   3641, 3615, 3573, 3577, 3684),      # Malignant Neoplasms\n",
        "                 ncol = 5, byrow = TRUE)\n",
        "n_years <- nrow(deaths)\n",
        "n_causes <- ncol(deaths)\n",
        "\n",
        "# Hyperparameters\n",
        "mu_prior_mean <- 0\n",
        "mu_prior_variance <- 100^2  # Weak prior\n",
        "alpha <- 0.001\n",
        "beta <- 0.001\n",
        "\n",
        "# Initialize parameters\n",
        "mu <- apply(deaths, 2, mean)  # Start with the sample mean for each cause\n",
        "sigma2 <- 1000  # Start with a large initial variance\n",
        "\n",
        "# Gibbs sampling settings\n",
        "n_iter <- 2000\n",
        "mu_samples <- matrix(NA, nrow = n_iter, ncol = n_causes)\n",
        "sigma2_samples <- numeric(n_iter)\n",
        "\n",
        "# Gibbs sampling loop\n",
        "for (iter in 1:n_iter) {\n",
        "  # Sample each mu_c | sigma^2, y\n",
        "  for (c in 1:n_causes) {\n",
        "    y_c <- deaths[, c]\n",
        "    n <- length(y_c)\n",
        "    mu_post_mean <- (sum(y_c) / sigma2 + mu_prior_mean / mu_prior_variance) /\n",
        "                    (n / sigma2 + 1 / mu_prior_variance)\n",
        "    mu_post_var <- 1 / (n / sigma2 + 1 / mu_prior_variance)\n",
        "    mu[c] <- rnorm(1, mean = mu_post_mean, sd = sqrt(mu_post_var))\n",
        "  }\n",
        "\n",
        "  # Sample sigma^2 | mu, y\n",
        "  residuals <- deaths - matrix(rep(mu, each = n_years), n_years, n_causes)\n",
        "  ss <- sum(residuals^2)\n",
        "  sigma2_post_shape <- alpha + 0.5 * n_years * n_causes\n",
        "  sigma2_post_rate <- beta + 0.5 * ss\n",
        "  sigma2 <- 1 / rgamma(1, shape = sigma2_post_shape, rate = sigma2_post_rate)\n",
        "\n",
        "  # Store the samples\n",
        "  mu_samples[iter, ] <- mu\n",
        "  sigma2_samples[iter] <- sigma2\n",
        "}\n",
        "\n",
        "# Results\n",
        "mu_samples_mean <- apply(mu_samples, 2, mean)\n",
        "sigma2_mean <- mean(sigma2_samples)\n",
        "\n",
        "print(\"Posterior means of mu for each cause:\")\n",
        "print(mu_samples_mean)\n",
        "\n",
        "print(\"Posterior mean of sigma^2:\")\n",
        "print(sigma2_mean)\n"
      ],
      "metadata": {
        "id": "96A25CMQyvUU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "827f331d-925f-473f-8cf2-a2b7fe72c0d3"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1] \"Posterior means of mu for each cause:\"\n",
            "[1]  9.575408  9.877249 12.709542  6.546121  6.945206\n",
            "[1] \"Posterior mean of sigma^2:\"\n",
            "[1] 264213076\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WZNF0bTZ1iGf"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract posterior samples of mu and sigma\n",
        "post_samples <- extract(fit_separate)\n",
        "mu_samples <- post_samples$mu\n",
        "sigma_samples <- post_samples$sigma\n",
        "\n",
        "# Predict the 6th cause\n",
        "predicted_mu_6 <- rnorm(1000, mean = mean(mu_samples), sd = mean(sigma_samples))\n",
        "hist(predicted_mu_6, breaks = 30, main = \"Predictive Distribution for 6th Cause\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 437
        },
        "id": "XShAFHe52TgQ",
        "outputId": "2b23cea5-de47-4294-c33b-77b3b33f4a8d"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Plot with title “Predictive Distribution for 6th Cause”"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAYAAAD958/bAAAACXBIWXMAABJ0AAASdAHeZh94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\nAAAAAAAAAAAAAAAAAAAAAAAAoGv+Hw2+0o72PY33AAAAAElFTkSuQmCC"
          },
          "metadata": {
            "image/png": {
              "width": 420,
              "height": 420
            }
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gibbs_separate <- function(y, num_samples, burn_in) {\n",
        "  N <- nrow(y)\n",
        "  C <- ncol(y)\n",
        "\n",
        "  # Initialize storage for samples\n",
        "  samples_mu <- matrix(NA, nrow = num_samples, ncol = C)\n",
        "  samples_sigma <- numeric(num_samples)\n",
        "\n",
        "  # Set initial values\n",
        "  samples_mu[1, ] <- colMeans(y)\n",
        "  samples_sigma[1] <- sd(as.numeric(y))\n",
        "\n",
        "  # Gibbs sampling\n",
        "  for (i in 2:num_samples) {\n",
        "    # Sample mu_i for each cause from Normal\n",
        "    for (j in 1:C) {\n",
        "      mu_post_mean <- mean(y[, j])\n",
        "      mu_post_sd <- samples_sigma[i-1] / sqrt(N)\n",
        "      samples_mu[i, j] <- rnorm(1, mu_post_mean, mu_post_sd)\n",
        "    }\n",
        "\n",
        "    # Sample sigma from the posterior (Inverse-Gamma)\n",
        "    residuals <- y - matrix(samples_mu[i, ], nrow = N, ncol = C, byrow = TRUE)\n",
        "    sigma_post_shape <- (N * C) / 2\n",
        "    sigma_post_scale <- sum(residuals^2) / 2\n",
        "    samples_sigma[i] <- sqrt(1 / rgamma(1, sigma_post_shape, sigma_post_scale))\n",
        "  }\n",
        "\n",
        "  # Discard burn-in samples\n",
        "  samples_mu <- samples_mu[(burn_in + 1):num_samples, ]\n",
        "  samples_sigma <- samples_sigma[(burn_in + 1):num_samples]\n",
        "\n",
        "  return(list(mu = samples_mu, sigma = samples_sigma))\n",
        "}\n",
        "\n",
        "# Run Gibbs Sampling for Separate Gaussian Model\n",
        "set.seed(123)\n",
        "num_samples <- 10000\n",
        "burn_in <- 1000\n",
        "separate_results <- gibbs_separate(y = data_list$y, num_samples = num_samples, burn_in = burn_in)\n",
        "\n",
        "# Posterior distributions for means of 5 causes\n",
        "separate_results$mu  # Posterior samples for each mu_i\n",
        "\n",
        "# Predictive distribution for a 6th cause\n",
        "# In the separate model, the 6th cause would not share any structure with the other causes\n",
        "mu_6_samples_separate <- rnorm(1000, mean = mean(separate_results$mu), sd = mean(separate_results$sigma))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "l4CCM_hT2ap5",
        "outputId": "a75ec41d-0b9c-406c-e805-c8bf1d963fa4"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table class=\"dataframe\">\n",
              "<caption>A matrix: 9000 × 5 of type dbl</caption>\n",
              "<tbody>\n",
              "\t<tr><td>19185.980</td><td>17436.0806</td><td>23497.619</td><td> 3740.1184</td><td>19315.019</td></tr>\n",
              "\t<tr><td>16995.633</td><td> 5639.2873</td><td>15385.758</td><td> 9945.3480</td><td> 2133.869</td></tr>\n",
              "\t<tr><td>14248.832</td><td> 5750.9795</td><td> 5584.717</td><td> 7566.1320</td><td>11780.936</td></tr>\n",
              "\t<tr><td> 3899.205</td><td>14848.9441</td><td>13156.048</td><td> 4661.5249</td><td> 6511.623</td></tr>\n",
              "\t<tr><td>11027.535</td><td> 2611.8800</td><td>11463.993</td><td> 1734.0963</td><td>24030.843</td></tr>\n",
              "\t<tr><td>12360.153</td><td>  487.1615</td><td>10425.785</td><td>-2888.7357</td><td> 2041.770</td></tr>\n",
              "\t<tr><td> 6342.027</td><td>15558.9181</td><td>10388.534</td><td> -909.3598</td><td>14299.753</td></tr>\n",
              "\t<tr><td> 2416.655</td><td> 7933.5814</td><td>20321.028</td><td> 3615.6042</td><td>13203.481</td></tr>\n",
              "\t<tr><td>11284.580</td><td>18009.5295</td><td>16381.460</td><td>13588.3654</td><td>18754.314</td></tr>\n",
              "\t<tr><td>11843.155</td><td>19836.5494</td><td>25763.163</td><td>19719.1495</td><td> 1348.076</td></tr>\n",
              "\t<tr><td>15038.816</td><td>25522.9500</td><td> 4695.606</td><td>20645.7648</td><td>14522.471</td></tr>\n",
              "\t<tr><td>10247.704</td><td>14536.1418</td><td>25078.543</td><td>10529.0409</td><td>18416.235</td></tr>\n",
              "\t<tr><td>10197.701</td><td>11679.3960</td><td>21284.033</td><td> 2303.7747</td><td>10027.627</td></tr>\n",
              "\t<tr><td>15660.602</td><td> 4733.0394</td><td>13942.706</td><td>12059.5248</td><td> 2334.024</td></tr>\n",
              "\t<tr><td> 7066.859</td><td>10812.7514</td><td>20901.873</td><td>13755.0098</td><td> 1880.610</td></tr>\n",
              "\t<tr><td> 7279.191</td><td> 8895.8123</td><td>17511.130</td><td> 4344.1310</td><td> 4576.439</td></tr>\n",
              "\t<tr><td> 3813.118</td><td>20965.2361</td><td>22768.242</td><td>13543.8449</td><td> 6817.192</td></tr>\n",
              "\t<tr><td> 5296.389</td><td> 2126.2635</td><td> 1161.890</td><td>10091.4452</td><td> 9198.099</td></tr>\n",
              "\t<tr><td>16300.549</td><td>22306.4300</td><td>15339.207</td><td>10283.2805</td><td> 7409.121</td></tr>\n",
              "\t<tr><td> 8298.974</td><td>14121.8016</td><td>13724.745</td><td> 4039.5582</td><td> 7582.113</td></tr>\n",
              "\t<tr><td> 9363.501</td><td> 7573.4976</td><td>15616.745</td><td> 9607.7433</td><td> 7160.957</td></tr>\n",
              "\t<tr><td> 9902.967</td><td>12856.5267</td><td>11622.157</td><td> 2258.5860</td><td> 9091.950</td></tr>\n",
              "\t<tr><td> 9113.838</td><td>16498.5535</td><td>21744.825</td><td> 3569.8441</td><td>12739.455</td></tr>\n",
              "\t<tr><td> 8818.772</td><td> 9272.3921</td><td> 9500.513</td><td> 9456.4259</td><td> 5829.705</td></tr>\n",
              "\t<tr><td>10156.441</td><td>10690.2951</td><td>18850.140</td><td>11098.9745</td><td> 6603.672</td></tr>\n",
              "\t<tr><td> 7505.040</td><td>12385.8643</td><td> 8339.160</td><td>14105.6839</td><td>12550.803</td></tr>\n",
              "\t<tr><td>12794.236</td><td> 8793.5404</td><td>19628.794</td><td> 8725.3820</td><td>13007.890</td></tr>\n",
              "\t<tr><td> 8833.263</td><td> 7806.5023</td><td>11738.737</td><td> 1404.5067</td><td>15420.436</td></tr>\n",
              "\t<tr><td>10918.680</td><td>20214.6542</td><td>23034.542</td><td>14273.3759</td><td>12088.630</td></tr>\n",
              "\t<tr><td>13455.948</td><td>17354.7444</td><td>20229.394</td><td> 5970.9149</td><td>12701.479</td></tr>\n",
              "\t<tr><td>⋮</td><td>⋮</td><td>⋮</td><td>⋮</td><td>⋮</td></tr>\n",
              "\t<tr><td> 4607.1377</td><td>10864.194</td><td>20385.670</td><td> 7692.8428</td><td> 7907.203</td></tr>\n",
              "\t<tr><td>10832.4877</td><td>10148.314</td><td>11431.581</td><td> 7323.3830</td><td>11506.284</td></tr>\n",
              "\t<tr><td> 3953.5850</td><td> 8182.858</td><td>19537.961</td><td> 9227.2527</td><td> 6365.049</td></tr>\n",
              "\t<tr><td>19046.0503</td><td>10951.196</td><td> 7032.983</td><td> 3437.8732</td><td>13353.172</td></tr>\n",
              "\t<tr><td>17987.9291</td><td> 8403.597</td><td>15100.133</td><td>  700.8233</td><td>12603.751</td></tr>\n",
              "\t<tr><td>  530.0536</td><td> 9145.954</td><td>20142.636</td><td> 6794.9993</td><td> 4653.580</td></tr>\n",
              "\t<tr><td>14701.5486</td><td>12968.004</td><td>26675.133</td><td> 9273.7613</td><td>12942.086</td></tr>\n",
              "\t<tr><td>11790.2758</td><td> 7205.127</td><td>23751.620</td><td> 8140.0222</td><td>16543.323</td></tr>\n",
              "\t<tr><td>13194.0974</td><td> 6761.456</td><td>17611.770</td><td> 2690.3255</td><td>12782.123</td></tr>\n",
              "\t<tr><td> 6567.8231</td><td> 7913.687</td><td> 7928.128</td><td> 3454.3863</td><td>16001.390</td></tr>\n",
              "\t<tr><td>12265.8024</td><td> 9889.547</td><td>15971.357</td><td>12147.2926</td><td> 8287.468</td></tr>\n",
              "\t<tr><td>14803.5123</td><td>21076.170</td><td>10387.359</td><td>16248.8225</td><td> 3990.750</td></tr>\n",
              "\t<tr><td> 3078.6038</td><td>12096.333</td><td>14128.856</td><td>14771.8417</td><td> 8092.768</td></tr>\n",
              "\t<tr><td> 5869.4312</td><td> 6289.070</td><td>15914.248</td><td> 7554.0021</td><td>-1403.541</td></tr>\n",
              "\t<tr><td> 9944.7709</td><td>12454.244</td><td>18909.680</td><td> 7251.2319</td><td>20551.976</td></tr>\n",
              "\t<tr><td>11937.0351</td><td>11102.176</td><td>19632.071</td><td>-1135.7499</td><td>11730.996</td></tr>\n",
              "\t<tr><td>12887.4935</td><td>16550.212</td><td>13896.081</td><td>10585.3145</td><td> 5574.453</td></tr>\n",
              "\t<tr><td>13695.7855</td><td>11118.208</td><td>20019.552</td><td>12941.0781</td><td>13165.516</td></tr>\n",
              "\t<tr><td>15846.5529</td><td>11114.796</td><td> 5492.574</td><td>17041.9619</td><td>12231.274</td></tr>\n",
              "\t<tr><td> 8544.6183</td><td>12172.630</td><td>16624.598</td><td>11651.9998</td><td>-1085.074</td></tr>\n",
              "\t<tr><td>13409.0008</td><td>18528.392</td><td>10275.750</td><td>11955.4245</td><td>15119.218</td></tr>\n",
              "\t<tr><td>12035.0788</td><td>16225.427</td><td>17979.261</td><td>16326.9440</td><td> 8738.758</td></tr>\n",
              "\t<tr><td> 6405.3275</td><td> 8903.242</td><td>11854.300</td><td> 8088.4965</td><td> 4777.740</td></tr>\n",
              "\t<tr><td> 3189.5585</td><td> 9538.890</td><td> 9332.800</td><td> 9996.2440</td><td>11089.192</td></tr>\n",
              "\t<tr><td> 9407.8703</td><td> 8606.270</td><td> 2083.079</td><td>19509.8935</td><td> 2311.121</td></tr>\n",
              "\t<tr><td>13537.2503</td><td>21619.286</td><td> 5428.208</td><td>19429.0096</td><td>20626.098</td></tr>\n",
              "\t<tr><td> 9447.4474</td><td>10849.081</td><td>18815.459</td><td>10775.5011</td><td> 2954.331</td></tr>\n",
              "\t<tr><td>17688.2554</td><td> 7881.115</td><td> 9930.707</td><td> 7633.0598</td><td>10543.261</td></tr>\n",
              "\t<tr><td>14317.8853</td><td>12031.534</td><td>14242.287</td><td> 8818.7207</td><td> 9430.144</td></tr>\n",
              "\t<tr><td>11529.9825</td><td>13752.229</td><td> 7526.118</td><td> 8591.9700</td><td>12101.840</td></tr>\n",
              "</tbody>\n",
              "</table>\n"
            ],
            "text/markdown": "\nA matrix: 9000 × 5 of type dbl\n\n| 19185.980 | 17436.0806 | 23497.619 |  3740.1184 | 19315.019 |\n| 16995.633 |  5639.2873 | 15385.758 |  9945.3480 |  2133.869 |\n| 14248.832 |  5750.9795 |  5584.717 |  7566.1320 | 11780.936 |\n|  3899.205 | 14848.9441 | 13156.048 |  4661.5249 |  6511.623 |\n| 11027.535 |  2611.8800 | 11463.993 |  1734.0963 | 24030.843 |\n| 12360.153 |   487.1615 | 10425.785 | -2888.7357 |  2041.770 |\n|  6342.027 | 15558.9181 | 10388.534 |  -909.3598 | 14299.753 |\n|  2416.655 |  7933.5814 | 20321.028 |  3615.6042 | 13203.481 |\n| 11284.580 | 18009.5295 | 16381.460 | 13588.3654 | 18754.314 |\n| 11843.155 | 19836.5494 | 25763.163 | 19719.1495 |  1348.076 |\n| 15038.816 | 25522.9500 |  4695.606 | 20645.7648 | 14522.471 |\n| 10247.704 | 14536.1418 | 25078.543 | 10529.0409 | 18416.235 |\n| 10197.701 | 11679.3960 | 21284.033 |  2303.7747 | 10027.627 |\n| 15660.602 |  4733.0394 | 13942.706 | 12059.5248 |  2334.024 |\n|  7066.859 | 10812.7514 | 20901.873 | 13755.0098 |  1880.610 |\n|  7279.191 |  8895.8123 | 17511.130 |  4344.1310 |  4576.439 |\n|  3813.118 | 20965.2361 | 22768.242 | 13543.8449 |  6817.192 |\n|  5296.389 |  2126.2635 |  1161.890 | 10091.4452 |  9198.099 |\n| 16300.549 | 22306.4300 | 15339.207 | 10283.2805 |  7409.121 |\n|  8298.974 | 14121.8016 | 13724.745 |  4039.5582 |  7582.113 |\n|  9363.501 |  7573.4976 | 15616.745 |  9607.7433 |  7160.957 |\n|  9902.967 | 12856.5267 | 11622.157 |  2258.5860 |  9091.950 |\n|  9113.838 | 16498.5535 | 21744.825 |  3569.8441 | 12739.455 |\n|  8818.772 |  9272.3921 |  9500.513 |  9456.4259 |  5829.705 |\n| 10156.441 | 10690.2951 | 18850.140 | 11098.9745 |  6603.672 |\n|  7505.040 | 12385.8643 |  8339.160 | 14105.6839 | 12550.803 |\n| 12794.236 |  8793.5404 | 19628.794 |  8725.3820 | 13007.890 |\n|  8833.263 |  7806.5023 | 11738.737 |  1404.5067 | 15420.436 |\n| 10918.680 | 20214.6542 | 23034.542 | 14273.3759 | 12088.630 |\n| 13455.948 | 17354.7444 | 20229.394 |  5970.9149 | 12701.479 |\n| ⋮ | ⋮ | ⋮ | ⋮ | ⋮ |\n|  4607.1377 | 10864.194 | 20385.670 |  7692.8428 |  7907.203 |\n| 10832.4877 | 10148.314 | 11431.581 |  7323.3830 | 11506.284 |\n|  3953.5850 |  8182.858 | 19537.961 |  9227.2527 |  6365.049 |\n| 19046.0503 | 10951.196 |  7032.983 |  3437.8732 | 13353.172 |\n| 17987.9291 |  8403.597 | 15100.133 |   700.8233 | 12603.751 |\n|   530.0536 |  9145.954 | 20142.636 |  6794.9993 |  4653.580 |\n| 14701.5486 | 12968.004 | 26675.133 |  9273.7613 | 12942.086 |\n| 11790.2758 |  7205.127 | 23751.620 |  8140.0222 | 16543.323 |\n| 13194.0974 |  6761.456 | 17611.770 |  2690.3255 | 12782.123 |\n|  6567.8231 |  7913.687 |  7928.128 |  3454.3863 | 16001.390 |\n| 12265.8024 |  9889.547 | 15971.357 | 12147.2926 |  8287.468 |\n| 14803.5123 | 21076.170 | 10387.359 | 16248.8225 |  3990.750 |\n|  3078.6038 | 12096.333 | 14128.856 | 14771.8417 |  8092.768 |\n|  5869.4312 |  6289.070 | 15914.248 |  7554.0021 | -1403.541 |\n|  9944.7709 | 12454.244 | 18909.680 |  7251.2319 | 20551.976 |\n| 11937.0351 | 11102.176 | 19632.071 | -1135.7499 | 11730.996 |\n| 12887.4935 | 16550.212 | 13896.081 | 10585.3145 |  5574.453 |\n| 13695.7855 | 11118.208 | 20019.552 | 12941.0781 | 13165.516 |\n| 15846.5529 | 11114.796 |  5492.574 | 17041.9619 | 12231.274 |\n|  8544.6183 | 12172.630 | 16624.598 | 11651.9998 | -1085.074 |\n| 13409.0008 | 18528.392 | 10275.750 | 11955.4245 | 15119.218 |\n| 12035.0788 | 16225.427 | 17979.261 | 16326.9440 |  8738.758 |\n|  6405.3275 |  8903.242 | 11854.300 |  8088.4965 |  4777.740 |\n|  3189.5585 |  9538.890 |  9332.800 |  9996.2440 | 11089.192 |\n|  9407.8703 |  8606.270 |  2083.079 | 19509.8935 |  2311.121 |\n| 13537.2503 | 21619.286 |  5428.208 | 19429.0096 | 20626.098 |\n|  9447.4474 | 10849.081 | 18815.459 | 10775.5011 |  2954.331 |\n| 17688.2554 |  7881.115 |  9930.707 |  7633.0598 | 10543.261 |\n| 14317.8853 | 12031.534 | 14242.287 |  8818.7207 |  9430.144 |\n| 11529.9825 | 13752.229 |  7526.118 |  8591.9700 | 12101.840 |\n\n",
            "text/latex": "A matrix: 9000 × 5 of type dbl\n\\begin{tabular}{lllll}\n\t 19185.980 & 17436.0806 & 23497.619 &  3740.1184 & 19315.019\\\\\n\t 16995.633 &  5639.2873 & 15385.758 &  9945.3480 &  2133.869\\\\\n\t 14248.832 &  5750.9795 &  5584.717 &  7566.1320 & 11780.936\\\\\n\t  3899.205 & 14848.9441 & 13156.048 &  4661.5249 &  6511.623\\\\\n\t 11027.535 &  2611.8800 & 11463.993 &  1734.0963 & 24030.843\\\\\n\t 12360.153 &   487.1615 & 10425.785 & -2888.7357 &  2041.770\\\\\n\t  6342.027 & 15558.9181 & 10388.534 &  -909.3598 & 14299.753\\\\\n\t  2416.655 &  7933.5814 & 20321.028 &  3615.6042 & 13203.481\\\\\n\t 11284.580 & 18009.5295 & 16381.460 & 13588.3654 & 18754.314\\\\\n\t 11843.155 & 19836.5494 & 25763.163 & 19719.1495 &  1348.076\\\\\n\t 15038.816 & 25522.9500 &  4695.606 & 20645.7648 & 14522.471\\\\\n\t 10247.704 & 14536.1418 & 25078.543 & 10529.0409 & 18416.235\\\\\n\t 10197.701 & 11679.3960 & 21284.033 &  2303.7747 & 10027.627\\\\\n\t 15660.602 &  4733.0394 & 13942.706 & 12059.5248 &  2334.024\\\\\n\t  7066.859 & 10812.7514 & 20901.873 & 13755.0098 &  1880.610\\\\\n\t  7279.191 &  8895.8123 & 17511.130 &  4344.1310 &  4576.439\\\\\n\t  3813.118 & 20965.2361 & 22768.242 & 13543.8449 &  6817.192\\\\\n\t  5296.389 &  2126.2635 &  1161.890 & 10091.4452 &  9198.099\\\\\n\t 16300.549 & 22306.4300 & 15339.207 & 10283.2805 &  7409.121\\\\\n\t  8298.974 & 14121.8016 & 13724.745 &  4039.5582 &  7582.113\\\\\n\t  9363.501 &  7573.4976 & 15616.745 &  9607.7433 &  7160.957\\\\\n\t  9902.967 & 12856.5267 & 11622.157 &  2258.5860 &  9091.950\\\\\n\t  9113.838 & 16498.5535 & 21744.825 &  3569.8441 & 12739.455\\\\\n\t  8818.772 &  9272.3921 &  9500.513 &  9456.4259 &  5829.705\\\\\n\t 10156.441 & 10690.2951 & 18850.140 & 11098.9745 &  6603.672\\\\\n\t  7505.040 & 12385.8643 &  8339.160 & 14105.6839 & 12550.803\\\\\n\t 12794.236 &  8793.5404 & 19628.794 &  8725.3820 & 13007.890\\\\\n\t  8833.263 &  7806.5023 & 11738.737 &  1404.5067 & 15420.436\\\\\n\t 10918.680 & 20214.6542 & 23034.542 & 14273.3759 & 12088.630\\\\\n\t 13455.948 & 17354.7444 & 20229.394 &  5970.9149 & 12701.479\\\\\n\t ⋮ & ⋮ & ⋮ & ⋮ & ⋮\\\\\n\t  4607.1377 & 10864.194 & 20385.670 &  7692.8428 &  7907.203\\\\\n\t 10832.4877 & 10148.314 & 11431.581 &  7323.3830 & 11506.284\\\\\n\t  3953.5850 &  8182.858 & 19537.961 &  9227.2527 &  6365.049\\\\\n\t 19046.0503 & 10951.196 &  7032.983 &  3437.8732 & 13353.172\\\\\n\t 17987.9291 &  8403.597 & 15100.133 &   700.8233 & 12603.751\\\\\n\t   530.0536 &  9145.954 & 20142.636 &  6794.9993 &  4653.580\\\\\n\t 14701.5486 & 12968.004 & 26675.133 &  9273.7613 & 12942.086\\\\\n\t 11790.2758 &  7205.127 & 23751.620 &  8140.0222 & 16543.323\\\\\n\t 13194.0974 &  6761.456 & 17611.770 &  2690.3255 & 12782.123\\\\\n\t  6567.8231 &  7913.687 &  7928.128 &  3454.3863 & 16001.390\\\\\n\t 12265.8024 &  9889.547 & 15971.357 & 12147.2926 &  8287.468\\\\\n\t 14803.5123 & 21076.170 & 10387.359 & 16248.8225 &  3990.750\\\\\n\t  3078.6038 & 12096.333 & 14128.856 & 14771.8417 &  8092.768\\\\\n\t  5869.4312 &  6289.070 & 15914.248 &  7554.0021 & -1403.541\\\\\n\t  9944.7709 & 12454.244 & 18909.680 &  7251.2319 & 20551.976\\\\\n\t 11937.0351 & 11102.176 & 19632.071 & -1135.7499 & 11730.996\\\\\n\t 12887.4935 & 16550.212 & 13896.081 & 10585.3145 &  5574.453\\\\\n\t 13695.7855 & 11118.208 & 20019.552 & 12941.0781 & 13165.516\\\\\n\t 15846.5529 & 11114.796 &  5492.574 & 17041.9619 & 12231.274\\\\\n\t  8544.6183 & 12172.630 & 16624.598 & 11651.9998 & -1085.074\\\\\n\t 13409.0008 & 18528.392 & 10275.750 & 11955.4245 & 15119.218\\\\\n\t 12035.0788 & 16225.427 & 17979.261 & 16326.9440 &  8738.758\\\\\n\t  6405.3275 &  8903.242 & 11854.300 &  8088.4965 &  4777.740\\\\\n\t  3189.5585 &  9538.890 &  9332.800 &  9996.2440 & 11089.192\\\\\n\t  9407.8703 &  8606.270 &  2083.079 & 19509.8935 &  2311.121\\\\\n\t 13537.2503 & 21619.286 &  5428.208 & 19429.0096 & 20626.098\\\\\n\t  9447.4474 & 10849.081 & 18815.459 & 10775.5011 &  2954.331\\\\\n\t 17688.2554 &  7881.115 &  9930.707 &  7633.0598 & 10543.261\\\\\n\t 14317.8853 & 12031.534 & 14242.287 &  8818.7207 &  9430.144\\\\\n\t 11529.9825 & 13752.229 &  7526.118 &  8591.9700 & 12101.840\\\\\n\\end{tabular}\n",
            "text/plain": [
              "      [,1]       [,2]       [,3]      [,4]       [,5]     \n",
              " [1,] 19185.980  17436.0806 23497.619  3740.1184 19315.019\n",
              " [2,] 16995.633   5639.2873 15385.758  9945.3480  2133.869\n",
              " [3,] 14248.832   5750.9795  5584.717  7566.1320 11780.936\n",
              " [4,]  3899.205  14848.9441 13156.048  4661.5249  6511.623\n",
              " [5,] 11027.535   2611.8800 11463.993  1734.0963 24030.843\n",
              " [6,] 12360.153    487.1615 10425.785 -2888.7357  2041.770\n",
              " [7,]  6342.027  15558.9181 10388.534  -909.3598 14299.753\n",
              " [8,]  2416.655   7933.5814 20321.028  3615.6042 13203.481\n",
              " [9,] 11284.580  18009.5295 16381.460 13588.3654 18754.314\n",
              "[10,] 11843.155  19836.5494 25763.163 19719.1495  1348.076\n",
              "[11,] 15038.816  25522.9500  4695.606 20645.7648 14522.471\n",
              "[12,] 10247.704  14536.1418 25078.543 10529.0409 18416.235\n",
              "[13,] 10197.701  11679.3960 21284.033  2303.7747 10027.627\n",
              "[14,] 15660.602   4733.0394 13942.706 12059.5248  2334.024\n",
              "[15,]  7066.859  10812.7514 20901.873 13755.0098  1880.610\n",
              "[16,]  7279.191   8895.8123 17511.130  4344.1310  4576.439\n",
              "[17,]  3813.118  20965.2361 22768.242 13543.8449  6817.192\n",
              "[18,]  5296.389   2126.2635  1161.890 10091.4452  9198.099\n",
              "[19,] 16300.549  22306.4300 15339.207 10283.2805  7409.121\n",
              "[20,]  8298.974  14121.8016 13724.745  4039.5582  7582.113\n",
              "[21,]  9363.501   7573.4976 15616.745  9607.7433  7160.957\n",
              "[22,]  9902.967  12856.5267 11622.157  2258.5860  9091.950\n",
              "[23,]  9113.838  16498.5535 21744.825  3569.8441 12739.455\n",
              "[24,]  8818.772   9272.3921  9500.513  9456.4259  5829.705\n",
              "[25,] 10156.441  10690.2951 18850.140 11098.9745  6603.672\n",
              "[26,]  7505.040  12385.8643  8339.160 14105.6839 12550.803\n",
              "[27,] 12794.236   8793.5404 19628.794  8725.3820 13007.890\n",
              "[28,]  8833.263   7806.5023 11738.737  1404.5067 15420.436\n",
              "[29,] 10918.680  20214.6542 23034.542 14273.3759 12088.630\n",
              "[30,] 13455.948  17354.7444 20229.394  5970.9149 12701.479\n",
              "[31,] ⋮          ⋮          ⋮         ⋮          ⋮        \n",
              "[32,]  4607.1377 10864.194  20385.670  7692.8428  7907.203\n",
              "[33,] 10832.4877 10148.314  11431.581  7323.3830 11506.284\n",
              "[34,]  3953.5850  8182.858  19537.961  9227.2527  6365.049\n",
              "[35,] 19046.0503 10951.196   7032.983  3437.8732 13353.172\n",
              "[36,] 17987.9291  8403.597  15100.133   700.8233 12603.751\n",
              "[37,]   530.0536  9145.954  20142.636  6794.9993  4653.580\n",
              "[38,] 14701.5486 12968.004  26675.133  9273.7613 12942.086\n",
              "[39,] 11790.2758  7205.127  23751.620  8140.0222 16543.323\n",
              "[40,] 13194.0974  6761.456  17611.770  2690.3255 12782.123\n",
              "[41,]  6567.8231  7913.687   7928.128  3454.3863 16001.390\n",
              "[42,] 12265.8024  9889.547  15971.357 12147.2926  8287.468\n",
              "[43,] 14803.5123 21076.170  10387.359 16248.8225  3990.750\n",
              "[44,]  3078.6038 12096.333  14128.856 14771.8417  8092.768\n",
              "[45,]  5869.4312  6289.070  15914.248  7554.0021 -1403.541\n",
              "[46,]  9944.7709 12454.244  18909.680  7251.2319 20551.976\n",
              "[47,] 11937.0351 11102.176  19632.071 -1135.7499 11730.996\n",
              "[48,] 12887.4935 16550.212  13896.081 10585.3145  5574.453\n",
              "[49,] 13695.7855 11118.208  20019.552 12941.0781 13165.516\n",
              "[50,] 15846.5529 11114.796   5492.574 17041.9619 12231.274\n",
              "[51,]  8544.6183 12172.630  16624.598 11651.9998 -1085.074\n",
              "[52,] 13409.0008 18528.392  10275.750 11955.4245 15119.218\n",
              "[53,] 12035.0788 16225.427  17979.261 16326.9440  8738.758\n",
              "[54,]  6405.3275  8903.242  11854.300  8088.4965  4777.740\n",
              "[55,]  3189.5585  9538.890   9332.800  9996.2440 11089.192\n",
              "[56,]  9407.8703  8606.270   2083.079 19509.8935  2311.121\n",
              "[57,] 13537.2503 21619.286   5428.208 19429.0096 20626.098\n",
              "[58,]  9447.4474 10849.081  18815.459 10775.5011  2954.331\n",
              "[59,] 17688.2554  7881.115   9930.707  7633.0598 10543.261\n",
              "[60,] 14317.8853 12031.534  14242.287  8818.7207  9430.144\n",
              "[61,] 11529.9825 13752.229   7526.118  8591.9700 12101.840"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "V4N-nW364M1G"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}